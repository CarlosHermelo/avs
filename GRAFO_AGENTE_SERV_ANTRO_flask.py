# üõ† Configuraci√≥n e Instalaci√≥n de Dependencias

import os
import getpass
import configparser
import logging
import sqlite3
import re
import numpy as np
import cohere

# --------------------- Configuraci√≥n de Logging ---------------------
glog_filename = 'script_log_antro.log'  # Nombre del archivo de log
logging.basicConfig(
    filename=glog_filename,
    filemode='a',  # Agregar registros al final del archivo existente
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO  # Nivel de logging (ajustable seg√∫n necesidad)
)

def log_message(message, level='DEBUG'):
    if level.upper() == 'INFO':
        logging.info(message)
    elif level.upper() == 'WARNING':
        logging.warning(message)
    elif level.upper() == 'ERROR':
        logging.error(message)
    elif level.upper() == 'DEBUG':
        logging.debug(message)
    else:
        logging.info(message)

# --------------------- Cargar Configuraci√≥n ---------------------
config = configparser.ConfigParser()
config.read('config.ini')

# Variables originales
collection_name_fragmento = config['DEFAULT'].get('collection_name_fragmento', fallback='fragment_store')
model_name = config['DEFAULT'].get('modelo')
log_message(f"Modelo configurado: {model_name}")

fragment_store_directory = config['SERVICIOS_SIMAP_ANTRO'].get('FRAGMENT_STORE_DIR', fallback='/content/chroma_fragment_store')
# Nota: Para la b√∫squeda sem√°ntica se usar√° 'max_results_chroma' (nueva variable)
max_results_chroma = config['SERVICIOS_SIMAP_ANTRO'].getint('max_results_chroma', fallback=50)
fecha_desde_pagina = config['SERVICIOS_SIMAP_ANTRO'].get('fecha_desde', fallback='2024-01-08')
fecha_hasta_pagina = config['SERVICIOS_SIMAP_ANTRO'].get('fecha_hasta', fallback='2024-12-10')

log_message(f"Fragment store directory: {fragment_store_directory}")
log_message(f"Max results (Chroma) configurado: {max_results_chroma}")
log_message(f"Rango de fechas configurado: {fecha_desde_pagina} a {fecha_hasta_pagina}")

# Variables para BM25 y reranking
bm25_db_path = config['SERVICIOS_SIMAP_ANTRO'].get('BM25_DB_PATH', 'bm25_index.db')
max_results_bm25 = config['SERVICIOS_SIMAP_ANTRO'].getint('max_results_bm25', fallback=100)
rerank_enabled = config['SERVICIOS_SIMAP_ANTRO'].getboolean('rerank_enabled', fallback=False)
rerank_top_n = config['SERVICIOS_SIMAP_ANTRO'].getint('rerank_top_n', fallback=150)
rerank_top_k = config['SERVICIOS_SIMAP_ANTRO'].getint('rerank_top_k', fallback=20)

# Configuraci√≥n de API Keys
api_key = config['DEFAULT'].get('openai_api_key')
os.environ['OPENAI_API_KEY'] = api_key
log_message("API Key OpenAI configurada.")

cohere_api_key = config['DEFAULT'].get('cohere_api_key', '').strip()
if cohere_api_key:
    co = cohere.Client(cohere_api_key)
    log_message("API Key Cohere configurada.")
else:
    co = None
    log_message("API Key Cohere no configurada, se deshabilitar√° el reranking.")

if "USER_AGENT" not in os.environ:
    os.environ["USER_AGENT"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    log_message("USER_AGENT configurado en las variables de entorno.")

# --------------------- Conexi√≥n a Chroma y Creaci√≥n de Embeddings ---------------------
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(api_key=os.environ['OPENAI_API_KEY'])
log_message("Embeddings creados con OpenAI.")

# Nota: Se reutiliza el vector store existente, aunque ahora se usar√° solo para la parte sem√°ntica (Chroma)
vector_store = Chroma(
    collection_name=collection_name_fragmento,
    persist_directory=fragment_store_directory,
    embedding_function=embeddings
)
log_message("Vector store cargado correctamente desde Chroma.")

# --------------------- Funciones Auxiliares de Texto ---------------------
def count_words(text):
    """Cuenta el n√∫mero de palabras en un texto."""
    return len(text.split())

def validar_palabras(prompt, max_palabras=1000000):
    """Valida si el prompt excede el l√≠mite de palabras permitido."""
    num_palabras = count_words(prompt)
    if num_palabras > max_palabras:
        log_message(f"El contenido supera el l√≠mite de {max_palabras} palabras ({num_palabras} utilizadas).")
        return False, num_palabras
    return True, num_palabras

def reducir_contenido_por_palabras(text, max_palabras=1000000):
    """Recorta el texto para no superar el l√≠mite de palabras."""
    palabras = text.split()
    if len(palabras) > max_palabras:
        log_message("El contenido es demasiado largo, truncando...")
        return " ".join(palabras[:max_palabras]) + "\n\n[Contenido truncado...]"
    return text

# --------------------- Nuevas Funciones de Recuperaci√≥n (Script 2) ---------------------
def clean_query(query):
    """Limpia la consulta eliminando caracteres especiales para FTS5."""
    return re.sub(r'[^\w\s]', '', query)

def retrieve_bm25(query):
    """B√∫squeda BM25 con SQLite FTS5."""
    log_message("üîç Consultando BM25...")
    results = []
    try:
        conn = sqlite3.connect(bm25_db_path)
        cursor = conn.cursor()
        safe_query = clean_query(query)
        cursor.execute(
            "SELECT chunk_content FROM chunks WHERE chunk_content MATCH ? LIMIT ?",
            (safe_query, max_results_bm25)
        )
        results = [{"content": row[0], "source": "BM25"} for row in cursor.fetchall()]
        conn.close()
        log_message(f"BM25 encontr√≥ {len(results)} resultados.")
    except Exception as e:
        log_message(f"‚ùå Error BM25: {str(e)}", level="ERROR")
    return results

def retrieve_chromadb(query):
    """B√∫squeda sem√°ntica con ChromaDB utilizando el vector store existente."""
    log_message("üîç Consultando ChromaDB...")
    results = []
    try:
        # Se reutiliza el vector_store creado anteriormente
        docs = vector_store.similarity_search_with_score(query, k=max_results_chroma)
        results = [{
            "content": doc.page_content,
            "score": score,
            "source": "ChromaDB"
        } for doc, score in docs]
        log_message(f"ChromaDB encontr√≥ {len(results)} resultados.")
    except Exception as e:
        log_message(f"‚ùå Error ChromaDB: {str(e)}", level="ERROR")
    return results


def rank_fusion(bm25_results, chroma_results):
    """Fusi√≥n h√≠brida mejorada usando Reciprocal Rank Fusion (RRF)"""
    #print("\nüîÑ Fusionando resultados con RRF...")
    combined = {}
    
    # Constante de suavizado para RRF (t√≠picamente entre 60-100)
    RRF_K = 60
    
    # Procesar resultados de ChromaDB con RRF
    for chroma_rank, chroma_res in enumerate(chroma_results, 1):
        key = chroma_res['content'][:150]  # Clave m√°s larga para mejor deduplicaci√≥n
        rrf_score = 1 / (chroma_rank + RRF_K)
        
        combined[key] = {
            **chroma_res,
            'rrf_score': rrf_score,
            'sources': ['ChromaDB']
        }

    # Procesar resultados BM25 con RRF y combinar
    for bm25_rank, bm25_res in enumerate(bm25_results, 1):
        key = bm25_res['content'][:150]
        rrf_score = 1 / (bm25_rank + RRF_K)
        
        if key in combined:
            combined[key]['rrf_score'] += rrf_score
            combined[key]['sources'].append('BM25')
        else:
            combined[key] = {
                **bm25_res,
                'rrf_score': rrf_score,
                'sources': ['BM25']
            }

    # Ordenar por puntuaci√≥n RRF combinada
    sorted_results = sorted(
        combined.values(), 
        key=lambda x: x['rrf_score'], 
        reverse=True
    )

    # Normalizar scores para mejor interpretaci√≥n
    max_score = max(r['rrf_score'] for r in sorted_results) if sorted_results else 1
    for res in sorted_results:
        res['score'] = res['rrf_score'] / max_score  # Normalizado 0-1
        res['source'] = ' + '.join(res['sources'])
        del res['rrf_score']
        del res['sources']

    return sorted_results[:rerank_top_n]

def cohere_rerank(query, documents):
    """Reorganizaci√≥n contextual con Cohere."""
    log_message("üéØ Reranking con Cohere...")
    if not co:
        log_message("‚ö†Ô∏è Cohere no configurado. Saltando reranking.")
        return documents
    try:
        texts = [doc['content'] for doc in documents]
        response = co.rerank(
            query=query,
            documents=texts,
            top_n=rerank_top_k,
            model='rerank-multilingual-v2.0'
        )
        # Reordenar documentos seg√∫n los √≠ndices devueltos por Cohere
        reranked = [documents[r.index] for r in response.results]
        log_message("Reranking completado con Cohere.")
        return reranked
    except Exception as e:
        log_message(f"‚ùå Error Cohere: {str(e)}", level="ERROR")
        return documents[:rerank_top_k]


def retrieve(query):
    """
    Recupera documentos relevantes utilizando una combinaci√≥n de BM25 y b√∫squeda sem√°ntica en ChromaDB.
    
    Par√°metros:
    query (str): Consulta ingresada por el usuario.

    Retorna:
    list: Lista de documentos ordenados seg√∫n relevancia.
    """
    # B√∫squedas paralelas
    bm25_res = retrieve_bm25(query)
    chroma_res = retrieve_chromadb(query)

    # Fusi√≥n h√≠brida mejorada
    fused = rank_fusion(bm25_res, chroma_res)

    
    log_message(f"FUSED...........> {fused}\nFIN FUSED \n\n ------------.")
   


    # Reranking contextual (se mantiene igual)
    if rerank_enabled and len(fused) > 1:
        fused = cohere_rerank(query, fused)

    # Resultados finales
    
    return fused



# --------------------- Grafo Conversacional (LangGraph) ---------------------
from langgraph.graph import MessagesState, StateGraph
from langgraph.prebuilt import ToolNode
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI  # Se utiliza el modelo de lenguaje configurado

llm = ChatOpenAI(model=model_name, temperature=0)  # Ajusta par√°metros seg√∫n sea necesario

# Nodo 1: Generar consulta o responder directamente
def query_or_respond(state: MessagesState):
    """Genera una consulta para la herramienta de recuperaci√≥n o responde directamente."""
    log_message("########### QUERY OR RESPOND ---------#####################")
    #llm_with_tools = llm.bind_tools([retrieve])
    llm_with_tools = llm.bind_tools(
        [retrieve], 
        tool_choice={"type": "function", "function": {"name": "retrieve"}}  # Forzar llamada
    )
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

# Nodo 2: Ejecutar la herramienta de recuperaci√≥n
tools = ToolNode([retrieve])

# Nodo 3: Generar la respuesta final
def generate(state: MessagesState):
    """Genera la respuesta final usando los documentos recuperados."""
    log_message("########### WEB-generate ---------#####################")
    recent_tool_messages = [msg for msg in reversed(state["messages"]) if msg.type == "tool"]
    
    # 1. Extraer y formatear documentos
    docs_content = "\n\n".join([
        f"üìÑ DOCUMENTO {i+1}:\n{msg.content}\n" 
        for i, msg in enumerate(recent_tool_messages[::-1])
    ])
    
    log_message(f"DOCUMENTOS RECUPERADOS:\n{docs_content}")
    # Validar si los documentos contienen t√©rminos clave de la pregunta
    user_question = state["messages"][0].content.lower()
    terms = user_question.split()
    
    if not any(term in docs_content.lower() for term in terms):
        return {"messages": [{"role": "assistant", "content": "Lo siento, no tengo informaci√≥n suficiente para responder esa pregunta."}]}
    
    # Construcci√≥n del mensaje del sistema con contexto y ejemplo (se mantiene el formato original)
     # Construcci√≥n del mensaje del sistema con contexto y ejemplo (se mantiene el formato original)
    system_message_content = ( """
<CONTEXTO>
La informaci√≥n proporcionada tiene como objetivo apoyar a los agentes que trabajan en las agencias de PAMI, quienes se encargan de atender las consultas de los afiliados. Este soporte est√° dise√±ado para optimizar la experiencia de atenci√≥n al p√∫blico y garantizar que los afiliados reciban informaci√≥n confiable y relevante en el menor tiempo posible.
</CONTEXTO>

<ROL>
   Eres un asistente virtual experto en los servicios y tr√°mites de PAMI.
</ROL>
<TAREA>
   Tu tarea es responder preguntas relacionadas con lo tr√°mites y servicios que ofrece la obra social PAMI, bas√°ndote √∫nicamente en los datos disponibles en la base de datos vectorial. Si la informaci√≥n no est√° disponible, debes decir 'No tengo esa informaci√≥n en este momento'.
</TAREA>

<MODO_RESPUESTA>
<EXPLICACI√ìN>
En tu respuesta debes:
-Ser breve y directa: Proporciona la informaci√≥n en un formato claro y conciso, enfoc√°ndote en los pasos esenciales o la acci√≥n principal que debe tomarse.
-Ser accionable: Prioriza el detalle suficiente para que el agente pueda transmitir la soluci√≥n al afiliado r√°pidamente o profundizar si es necesario.
-Evitar informaci√≥n innecesaria: Incluye solo los datos m√°s relevantes para resolver la consulta. Si hay pasos opcionales o detalles adicionales, ind√≠calos solo si son cr√≠ticos.
-Estructura breve: Usa puntos clave, numeraci√≥n o listas de una sola l√≠nea si es necesario.
-El contenido de la respuesta debe estar orientado a lo que debe hacer el Afiliado 
</EXPLICACION> 

   <EJEMPLO_MODO_RESPUESTA>
      <PREGUNTA>
         ¬øC√≥mo tramitar la insulina tipo glargina?
      </PREGUNTA>
      <RESPUESTA>
         PAMI cubre al 100% la insulina tipo Glargina para casos especiales, previa autorizaci√≥n por v√≠a de excepci√≥n. Para gestionarla, se debe presentar el Formulario de Insulinas por V√≠a de Excepci√≥n (INICIO o RENOVACI√ìN) firmado por el m√©dico especialista, acompa√±ado de los √∫ltimos dos an√°lisis de sangre completos (hemoglobina glicosilada y glucemia, firmados por un bioqu√≠mico), DNI, credencial de afiliaci√≥n y receta electr√≥nica. La solicitud se presenta en la UGL o agencia de PAMI y ser√° evaluada por Nivel Central en un plazo de 72 horas. La autorizaci√≥n tiene una vigencia de 12 meses.
         
      </RESPUESTA>
   </EJEMPLO_MODO_RESPUESTA>
</MODO_RESPUESTA>

<CASOS_DE_PREGUNTA_RESPUESTA>
        <REQUISITOS>
        Si la respuesta tiene requisitos listar **TODOS** los requisitos encontrados en el contexto no omitas      incluso si aparecen en chunks distintos o al final de un fragmento. 
**Ejemplo cr√≠tico**: Si un chunk menciona "DNI, recibo, credencial" y otro agrega "Boleta de luz ", DEBEN incluirse ambos.
                             
         **Advertencia**:
          Si faltan requisitos del contexto en tu respuesta, se considerar√° ERROR GRAVE.                         
        </REQUISITOS>
       
   <IMPORTANTES_Y_EXCEPCIONES>
      Si los servicios o tr√°mites tienen EXCEPCIONES, aclaraciones o detalles IMPORTANTES, EXCLUSIONES, menci√≥nalos en tu respuesta.
        <EJEMPLO>
           ### Exclusiones:
            Afiliados internados en geriaticos privados
           ### Importante
            La orden tiene un vencimiento de 90 dias
           ### Excepciones
            Las solicitudes por vulnerabilidad no tendr√°n vencimiento
        </EJEMPLO>                      
   </IMPORTANTES_Y_EXCEPCIONES>

   <TRAMITES_NO_DISPONIBLES>
      <EXPLICACION>
         Si la pregunta es sobre un tr√°mite o servicio que no est√° expl√≠citamente indicado en la base de datos vectorial, menciona que no existe ese tr√°mite o servicio.
      </EXPLICACION>
      <EJEMPLO>
         <PREGUNTA>
            ¬øC√≥mo puede un afiliado solicitar un descuento por anteojos?
         </PREGUNTA>
         <RESPUESTA>
            PAMI no brinda un descuento por anteojos,Por lo tanto, si el afiliado decide comprar los anteojos por fuera de la red de √≥pticas de PAMI, no ser√° posible solicitar un reintegro.
         </RESPUESTA>
      </EJEMPLO>
   </TRAMITES_NO_DISPONIBLES>

   <CALCULOS_NUMERICOS>
      <EXPLICACION>
         Si la pregunta involucra un c√°lculo o comparaci√≥n num√©rica, eval√∫a aritm√©ticamente para responderla.
      </EXPLICACION>
      <EJEMPLO>
         - Si se dice "menor a 10", es un n√∫mero entre 1 y 9.
         - Si se dice "23", es un n√∫mero entre 21 y 24.
      </EJEMPLO>
   </CALCULOS_NUMERICOS>

   <FORMATO_RESPUESTA>
      <EXPLICACION>
         Presenta la informaci√≥n en formato de lista Markdown si es necesario.
      </EXPLICACION>
   </FORMATO_RESPUESTA>

   <REFERENCIAS>
      <EXPLICACION>
         Al final de tu respuesta, incluye siempre un apartado titulado **Referencias** que contenga combinaciones √∫nicas de **ID_SUB** y **SUBTIPO**, m√°s un link con la siguiente estructura:
      </EXPLICACION>
      <EJEMPLO>
         Referencias:
         - ID_SUB = 347 | SUBTIPO = 'Traslados Programados'
         - LINK = https://simap.pami.org.ar/subtipo_detalle.php?id_sub=347
      </EJEMPLO>
   </REFERENCIAS>
</CASOS_DE_PREGUNTA_RESPUESTA>

"""
    + docs_content 
     )
    
  

    system_message_content = ( """
<CONTEXTO>
La informaci√≥n proporcionada tiene como objetivo apoyar a los agentes que trabajan en las agencias de PAMI, quienes se encargan de atender las consultas de los afiliados. Este soporte est√° dise√±ado para optimizar la experiencia de atenci√≥n al p√∫blico y garantizar que los afiliados reciban informaci√≥n confiable y relevante en el menor tiempo posible.
</CONTEXTO>

<ROL>
   Eres un asistente virtual experto en los servicios y tr√°mites de PAMI.
</ROL>
<TAREA>
   Tu tarea es responder preguntas relacionadas con lo tr√°mites y servicios que ofrece la obra social PAMI, bas√°ndote √∫nicamente en los datos disponibles en la base de datos vectorial. Si la informaci√≥n no est√° disponible, debes decir 'No tengo esa informaci√≥n en este momento'.
</TAREA>

<MODO_RESPUESTA>
<EXPLICACI√ìN>
En tu respuesta debes:
-Ser breve y directa: Proporciona la informaci√≥n en un formato claro y conciso, enfoc√°ndote en los pasos esenciales o la acci√≥n principal que debe tomarse.
-Ser accionable: Prioriza el detalle suficiente para que el agente pueda transmitir la soluci√≥n al afiliado r√°pidamente o profundizar si es necesario.
-Evitar informaci√≥n innecesaria: Incluye solo los datos m√°s relevantes para resolver la consulta. Si hay pasos opcionales o detalles adicionales, ind√≠calos solo si son cr√≠ticos.
-Estructura breve: Usa puntos clave, numeraci√≥n o listas de una sola l√≠nea si es necesario.
-El contenido de la respuesta debe estar orientado a lo que debe hacer el Afiliado 
                              
-Es importante indicar donde se realiza el tramite si en la Agencia, en la web ,etc
</EXPLICACION> 

   <EJEMPLO_MODO_RESPUESTA>
        <PREGUNTA1>                       
      <PREGUNTA2>
         ¬øC√≥mo tramitar la insulina tipo glargina?
      </PREGUNTA2>
      <RESPUESTA2>
        PAMI cubre al 100% la insulina tipo Glargina para casos especiales, previa autorizaci√≥n por v√≠a de excepci√≥n. 
        Para gestionarla el AFILIADO , debe presentar:
1. Formulario de Insulinas por V√≠a de Excepci√≥n (INICIO o RENOVACI√ìN) firmado por el m√©dico especialista.
2. √öltimos dos an√°lisis de sangre completos (hemoglobina glicosilada y glucemia), firmados por un bioqu√≠mico.
....

##### Donde se realiza el tr√°mite
-El tr√°mite se reaiza en forma presencial en la UGL
### Importante
- EL afiliado debe estar registrado previamente en el Padr√≥n de personas con Diabetes.

      </RESPUESTA>
   </EJEMPLO_MODO_RESPUESTA>
</MODO_RESPUESTA>

<CASOS_DE_PREGUNTA_RESPUESTA>
        <REQUISITOS>
        Si la respuesta tiene requisitos listar **TODOS** los requisitos encontrados en el contexto no omitas      incluso si aparecen en chunks distintos o al final de un fragmento. 
**Ejemplo cr√≠tico**: Si un chunk menciona "DNI, recibo, credencial" y otro agrega "Boleta de luz ", DEBEN incluirse ambos.
                             
         **Advertencia**:
          Si faltan requisitos del contexto en tu respuesta, se considerar√° ERROR GRAVE.                         
        </REQUISITOS>
       
   <IMPORTANTES_Y_EXCEPCIONES>
      Si los servicios o tr√°mites tienen EXCEPCIONES, aclaraciones o detalles IMPORTANTES, EXCLUSIONES, menci√≥nalos en tu respuesta.
        <EJEMPLO>
           ### Exclusiones:
            Afiliados internados en geriaticos privados
           ### Importante
            La orden tiene un vencimiento de 90 dias
           ### Excepciones
            Las solicitudes por vulnerabilidad no tendr√°n vencimiento
        </EJEMPLO>                      
   </IMPORTANTES_Y_EXCEPCIONES>

   <TRAMITES_NO_DISPONIBLES>
      <EXPLICACION>
         Si la pregunta es sobre un tr√°mite o servicio que no est√° expl√≠citamente indicado en la base de datos vectorial, menciona que no existe ese tr√°mite o servicio.
      </EXPLICACION>
      <EJEMPLO>
         <PREGUNTA>
            ¬øC√≥mo puede un afiliado solicitar un descuento por anteojos?
         </PREGUNTA>
         <RESPUESTA>
            PAMI no brinda un descuento por anteojos,Por lo tanto, si el afiliado decide comprar los anteojos por fuera de la red de √≥pticas de PAMI, no ser√° posible solicitar un reintegro.
         </RESPUESTA>
      </EJEMPLO>
   </TRAMITES_NO_DISPONIBLES>

   <CALCULOS_NUMERICOS>
      <EXPLICACION>
         Si la pregunta involucra un c√°lculo o comparaci√≥n num√©rica, eval√∫a aritm√©ticamente para responderla.
      </EXPLICACION>
      <EJEMPLO>
         - Si se dice "menor a 10", es un n√∫mero entre 1 y 9.
         - Si se dice "23", es un n√∫mero entre 21 y 24.
      </EJEMPLO>
   </CALCULOS_NUMERICOS>

   <FORMATO_RESPUESTA>
      <EXPLICACION>
         Presenta la informaci√≥n en formato de lista Markdown si es necesario.
      </EXPLICACION>
   </FORMATO_RESPUESTA>

   <REFERENCIAS>
      <EXPLICACION>
         Al final de tu respuesta, incluye siempre un apartado titulado **Referencias** que contenga combinaciones √∫nicas de **ID_SUB** y **SUBTIPO**, m√°s un link con la siguiente estructura:
      </EXPLICACION>
      <EJEMPLO>
         Referencias:
         - ID_SUB = 347 | SUBTIPO = 'Traslados Programados'
         - LINK = https://simap.pami.org.ar/subtipo_detalle.php?id_sub=347
      </EJEMPLO>
   </REFERENCIAS>
</CASOS_DE_PREGUNTA_RESPUESTA>

"""
    + docs_content 
     )
    
    # Validar l√≠mite de palabras
    es_valido, num_palabras = validar_palabras(system_message_content)
    if not es_valido:
        system_message_content = reducir_contenido_por_palabras(system_message_content)
        log_message(f"Se redujo el contenido a {count_words(system_message_content)} palabras.")
    
    # Loguear el contexto que se usar√° en el prompt
    log_message("Contexto de prompt para consulta:\n" + system_message_content)
    
    prompt = [SystemMessage(system_message_content)] + [
        msg for msg in state["messages"] if msg.type in ("human", "system")
    ]

# Filtrar solo el √∫ltimo mensaje humano y cualquier system message relevante
    human_messages = [msg for msg in state["messages"] if msg.type == "human"]
    last_human_message = human_messages[-1] if human_messages else None

    system_messages = [msg for msg in state["messages"] if msg.type == "system"]

    prompt = [
        SystemMessage(content=system_message_content),  # Nuevo sistema con contexto
        last_human_message,  # √öltima pregunta
        *system_messages  # Mantener system messages existentes si son cr√≠ticos
    ]

    prompt = [
        SystemMessage(content=system_message_content),
        HumanMessage(content=state["messages"][0].content)  # La pregunta original
    ]
    # Debug: Verificar prompt completo
    
    
    log_message(f"\n\n\nWEB-PROMPTJJJJ: {prompt} \n\n---FIN WEB-PROMPT")
    response = llm.invoke(prompt)
    log_message(f"Respuesta del LLM: {response}")
    return {"messages": [response]}

# Construcci√≥n y conexi√≥n del grafo
graph_builder = StateGraph(MessagesState)
graph_builder.add_node(query_or_respond)
graph_builder.add_node(tools)
graph_builder.add_node(generate)
graph_builder.set_entry_point("query_or_respond")
graph_builder.add_edge("query_or_respond", "tools")
graph_builder.add_edge("tools", "generate")
graph = graph_builder.compile()

# --------------------- Funci√≥n para Procesar Preguntas ---------------------
def process_question(question_input: str, fecha_desde: str, fecha_hasta: str, k: int):
    log_message("############## Iniciando process_question ##############")
    from langgraph.checkpoint.memory import MemorySaver
    memory = MemorySaver()
    graph = graph_builder.compile(checkpointer=memory)
    try:
        for step in graph.stream(
            {"messages": [{"role": "user", "content": question_input}]},
            stream_mode="values",
            config={"configurable": {"thread_id": "user_question"}},
        ):
            response = step["messages"][-1].content
            log_message("############## Fin process_question jaja ##############")
        return response
    except Exception as e:
        log_message(f"Error en process_question: {str(e)}", level="ERROR")
        return f"Error: {str(e)}"

# --------------------------------------------------------------------
# Fin del script mejorado
# --------------------------------------------------------------------

# Custo v-1.0.3
